{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038a026b",
   "metadata": {
    "papermill": {
     "duration": 0.009323,
     "end_time": "2024-08-23T01:22:36.134612",
     "exception": false,
     "start_time": "2024-08-23T01:22:36.125289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "86699bfc",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-06T18:11:11.353635Z",
     "iopub.status.busy": "2024-07-06T18:11:11.352993Z",
     "iopub.status.idle": "2024-07-06T18:13:02.168453Z",
     "shell.execute_reply": "2024-07-06T18:13:02.167132Z",
     "shell.execute_reply.started": "2024-07-06T18:11:11.353600Z"
    },
    "papermill": {
     "duration": 0.008892,
     "end_time": "2024-08-23T01:22:36.152474",
     "exception": false,
     "start_time": "2024-08-23T01:22:36.143582",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "!pip3 install -q -U bitsandbytes==0.42.0\n",
    "!pip3 install -q -U peft==0.8.2\n",
    "!pip3 install -q -U trl==0.7.10\n",
    "!pip3 install -q -U accelerate==0.27.1\n",
    "!pip3 install -q -U datasets==2.17.0\n",
    "!pip3 install -q -U transformers==4.38.0\n",
    "!pip install -q -U optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "364cbdc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T01:22:36.172392Z",
     "iopub.status.busy": "2024-08-23T01:22:36.171510Z",
     "iopub.status.idle": "2024-08-23T01:22:57.478717Z",
     "shell.execute_reply": "2024-08-23T01:22:57.476919Z"
    },
    "papermill": {
     "duration": 21.31964,
     "end_time": "2024-08-23T01:22:57.480924",
     "exception": true,
     "start_time": "2024-08-23T01:22:36.161284",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "Connection error trying to communicate with service.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/urllib/request.py:1348\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1348\u001b[0m     \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1349\u001b[0m \u001b[43m              \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_header\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTransfer-encoding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:1283\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1283\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:1329\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1328\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1329\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:1278\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1278\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:1038\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1038\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1041\u001b[0m \n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:976\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m--> 976\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:1448\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnect to a host on a given (SSL) port.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:942\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    941\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp.client.connect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport)\n\u001b[0;32m--> 942\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;66;03m# Might fail in OSs that don't implement TCP_NODELAY\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/socket.py:824\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    823\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 824\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    825\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/socket.py:955\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    954\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 955\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    956\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno -3] Temporary failure in name resolution",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/kaggle_web_client.py:46\u001b[0m, in \u001b[0;36mKaggleWebClient.make_post_request\u001b[0;34m(self, data, endpoint, timeout)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[1;32m     47\u001b[0m         response_json \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mread())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/urllib/request.py:519\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    518\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[0;32m--> 519\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/urllib/request.py:536\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    535\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[0;32m--> 536\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m    537\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_open\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    495\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/urllib/request.py:1391\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[0;32m-> 1391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/urllib/request.py:1351\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[0;32m-> 1351\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[1;32m   1352\u001b[0m r \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mgetresponse()\n",
      "\u001b[0;31mURLError\u001b[0m: <urlopen error [Errno -3] Temporary failure in name resolution>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkaggle_secrets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UserSecretsClient\n\u001b[1;32m      2\u001b[0m secret_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHF_TOKEN\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m secret_value \u001b[38;5;241m=\u001b[39m \u001b[43mUserSecretsClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_secret\u001b[49m\u001b[43m(\u001b[49m\u001b[43msecret_label\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/kaggle_secrets.py:64\u001b[0m, in \u001b[0;36mUserSecretsClient.get_secret\u001b[0;34m(self, label)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ValidationError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel must be non-empty.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     61\u001b[0m request_body \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m'\u001b[39m: label,\n\u001b[1;32m     63\u001b[0m }\n\u001b[0;32m---> 64\u001b[0m response_json \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweb_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_post_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET_USER_SECRET_BY_LABEL_ENDPOINT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msecret\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m response_json:\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BackendError(\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnexpected response from the service. Response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse_json\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/kaggle_web_client.py:58\u001b[0m, in \u001b[0;36mKaggleWebClient.make_post_request\u001b[0;34m(self, data, endpoint, timeout)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m     54\u001b[0m             e, socket\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m     55\u001b[0m             e\u001b[38;5;241m.\u001b[39mreason, socket\u001b[38;5;241m.\u001b[39mtimeout):\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\n\u001b[1;32m     57\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTimeout error trying to communicate with service. Please ensure internet is on.\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConnection error trying to communicate with service.\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m e\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n",
      "\u001b[0;31mConnectionError\u001b[0m: Connection error trying to communicate with service."
     ]
    }
   ],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "secret_label = \"HF_TOKEN\"\n",
    "secret_value = UserSecretsClient().get_secret(secret_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18c10ed",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"training_config\": {\n",
    "        \"per_device_train_batch_size\":6,\n",
    "        \"gradient_accumulation_steps\":4,\n",
    "        \"warmup_steps\":0.03,\n",
    "        \"max_steps\":84,\n",
    "        \"learning_rate\":8.8e-5,\n",
    "        \"logging_steps\": 1\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b40b66",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "#set the qunatization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "#\n",
    "#Load the model and Tokenizer\n",
    "model_id = \"Qwen/Qwen2-1.5B-Instruct\"\n",
    "#\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d019413",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config_dataset={    \"training_dataset\": {\n",
    "        \"name\": \"ruslanmv/ai-medical-dataset\", # The dataset name(huggingface/datasets)\n",
    "        \"split\": \"train\",  # The dataset split\n",
    "        \"input_fields\": [\"question\", \"context\"] ,# The input fields\n",
    "        \"input_field\": \"text\",# The input field\n",
    "    },\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496a64f9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config_dataset.get(\"training_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf65e232",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# Loading the training dataset\n",
    "train_dataset = load_dataset(config_dataset.get(\"training_dataset\").get(\"name\"), split = config_dataset.get(\"training_dataset\").get(\"split\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1785fa70",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7909203e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = train_dataset.select(range(100))\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359b7834",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52950bc4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "medical_prompt = \"\"\"You are an AI Medical Assistant Chatbot, trained to answer medical questions. Below is an instruction that describes a task, paired with an response context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"question\"]\n",
    "    outputs      = examples[\"context\"]\n",
    "    texts = []\n",
    "    for instruction, output in zip(instructions,  outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = medical_prompt.format(instruction,  output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87594f85",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset= test_dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494bb3d3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a8ba52",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1325b54",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "is_test=True\n",
    "if is_test:\n",
    "    train_dataset=test_dataset\n",
    "else:\n",
    "    train_dataset= train_dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be30e02f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c34fa8e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "def find_all_linear_names(model):\n",
    "  cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "  lora_module_names = set()\n",
    "  for name, module in model.named_modules():\n",
    "    if isinstance(module, cls):\n",
    "      names = name.split('.')\n",
    "      lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "      lora_module_names.remove('lm_head')\n",
    "  return list(lora_module_names)\n",
    "#\n",
    "modules = find_all_linear_names(model)\n",
    "print(modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca63d12f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "#\n",
    "print(model)\n",
    "\n",
    "#\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=32,\n",
    "    target_modules=modules,\n",
    "    lora_dropout=0.091,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95408197",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainable, total = model.get_nb_trainable_parameters()\n",
    "print(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cf3118",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side='right'\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    dataset_text_field=config_dataset.get(\"training_dataset\").get(\"input_field\"),\n",
    "    peft_config=lora_config,\n",
    "    max_seq_length=2500,\n",
    "    dataset_num_proc = 2,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=config.get(\"training_config\").get(\"per_device_train_batch_size\"),\n",
    "        gradient_accumulation_steps=config.get(\"training_config\").get(\"gradient_accumulation_steps\"),\n",
    "        warmup_steps=config.get(\"training_config\").get(\"warmup_steps\"),\n",
    "        max_steps=config.get(\"training_config\").get(\"max_steps\"),\n",
    "        learning_rate=config.get(\"training_config\").get(\"learning_rate\"),\n",
    "        logging_steps=config.get(\"training_config\").get(\"logging_steps\"),\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        save_strategy=\"epoch\",\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "#\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7e879c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##USE THIS TO FINE TUNE HYPERPARAMS USING OPTUNA. YOU CAN ADD MORE HYPERPARAMS IN THE SEARCH SPACE FOR FINE TUNING.\n",
    "## GIVEN BELOW ARE THE BEST HYPERPARAMS FOUND AFTER RUNNING THIS EXPERIMENT.\n",
    "# Best Hyperparameters: {'learning_rate': 8.834086710159391e-05, 'per_device_train_batch_size': 6, 'gradient_accumulation_steps': 4, 'lora_alpha': 32, 'lora_dropout': 0.09057747591882871, 'max_steps': 84.35209460217322}\n",
    "# Best Training Loss: 0.03177290491759777\n",
    "    \n",
    "    \n",
    "# from optuna import create_study, Trial\n",
    "\n",
    "\n",
    "# search_space = {\n",
    "#     \"per_device_train_batch_size\":[1, 2, 4, 8],\n",
    "#     \"learning_rate\": [1e-5, 5e-5, 1e-4, 2e-4],\n",
    "#     \"lora_alpha\": [8, 16, 32],\n",
    "#     \"max_steps\": [50, 100, 150],\n",
    "#     \"gradient_accumulation_steps\": [1, 2, 4, 8],\n",
    "#     \"lora_dropout\": [0.05, 0.1, 0.2 ]\n",
    "# }\n",
    "\n",
    "# def objective(trial):\n",
    "#     # set hyperparams based on trial values\n",
    "#     config[\"training_config\"][\"learning_rate\"] = trial.suggest_float(\"learning_rate\", search_space[\"learning_rate\"][0], search_space[\"learning_rate\"][-1])\n",
    "#     config[\"training_config\"][\"per_device_train_batch_size\"] = trial.suggest_int(\"per_device_train_batch_size\", search_space[\"per_device_train_batch_size\"][0], search_space[\"per_device_train_batch_size\"][-1])\n",
    "#     config[\"training_config\"][\"gradient_accumulation_steps\"] = trial.suggest_int(\"gradient_accumulation_steps\", search_space[\"gradient_accumulation_steps\"][0], search_space[\"gradient_accumulation_steps\"][-1])\n",
    "#     lora_config.lora_alpha = trial.suggest_int(\"lora_alpha\", search_space[\"lora_alpha\"][0], search_space[\"lora_alpha\"][-1])\n",
    "#     lora_config.lora_dropout = trial.suggest_float(\"lora_dropout\", search_space[\"lora_dropout\"][0], search_space[\"lora_dropout\"][-1])\n",
    "#     config[\"training_config\"][\"max_steps\"] = trial.suggest_float(\"max_steps\", search_space[\"max_steps\"][0], search_space[\"max_steps\"][-1])\n",
    "\n",
    "#     trainer_stats = trainer.train()\n",
    "#     return trainer_stats[1]\n",
    "    \n",
    "\n",
    "# study = create_study(direction=\"minimize\")\n",
    "# study.optimize(objective, n_trials=16)  # Adjust the number of trials\n",
    "\n",
    "# # Access the best trial and its hyperparameters after optimization\n",
    "# best_trial = study.best_trial\n",
    "# best_params = best_trial.params\n",
    "\n",
    "# print(\"Best Trial:\", best_trial.number)\n",
    "# print(\"Best Hyperparameters:\", best_params)\n",
    "# print(\"Best Training Loss:\", best_trial.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37377b2f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(secret_value, add_to_git_credential = True)\n",
    "\n",
    "new_model = \"qwen2-1.5B-medical_qa-Finetune\" \n",
    "#\n",
    "trainer.model.save_pretrained(new_model)\n",
    "#\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "merged_model= PeftModel.from_pretrained(base_model, new_model)\n",
    "merged_model= merged_model.merge_and_unload()\n",
    "\n",
    "# Save the merged model\n",
    "#save_adapter=True, save_config=True\n",
    "merged_model.save_pretrained(\"merged_model\",safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"merged_model\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "#\n",
    "# Push the model and tokenizer to the Hugging Face Model Hub\n",
    "merged_model.config.to_json_file(\"adapter_config.json\")\n",
    "merged_model.push_to_hub(new_model, use_temp_dir=False)\n",
    "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f6f3b9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_completion(query: str, model, tokenizer) -> str:\n",
    "  device = \"cuda:0\"\n",
    "\n",
    "  prompt_template = \"\"\"\n",
    "  <start_of_turn>user\n",
    "  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "  {query}\n",
    "  <end_of_turn>\\n<start_of_turn>model\n",
    "\n",
    "\n",
    "  \"\"\"\n",
    "  prompt = prompt_template.format(query=query)\n",
    "\n",
    "  encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "  model_inputs = encodeds.to(device)\n",
    "\n",
    "\n",
    "  generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "  # decoded = tokenizer.batch_decode(generated_ids)\n",
    "  decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "  return (decoded)\n",
    "#\n",
    "query = \"\"\"\\n\\n Please answer with one of the option in the bracket. Write reasoning in between <analysis></analysis>. Write answer in between <answer></answer>. here are the inputs Q:An 8-year-old boy is brought to the pediatrician by his mother with nausea, vomiting, and decreased frequency of urination. He has acute lymphoblastic leukemia for which he received the 1st dose of chemotherapy 5 days ago. His leukocyte count was 60,000/mm3 before starting chemotherapy. The vital signs include: pulse 110/min, temperature 37.0°C (98.6°F), and blood pressure 100/70 mm Hg. The physical examination shows bilateral pedal edema. Which of the following serum studies and urinalysis findings will be helpful in confirming the diagnosis of this condition? ? \\n{'A': 'Hyperkalemia, hyperphosphatemia, hypocalcemia, and extremely elevated creatine kinase (MM)', 'B': 'Hyperkalemia, hyperphosphatemia, hypocalcemia, hyperuricemia, urine supernatant pink, and positive for heme', 'C': 'Hyperuricemia, hyperkalemia, hyperphosphatemia, lactic acidosis, and urate crystals in the urine', 'D': 'Hyperuricemia, hyperkalemia, hyperphosphatemia, and urinary monoclonal spike', 'E': 'Hyperuricemia, hyperkalemia, hyperphosphatemia, lactic acidosis, and oxalate crystals'}\"\"\"\n",
    "\n",
    "result = get_completion(query=query, model=merged_model, tokenizer=tokenizer)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5627638d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Model Answer : \\n {result.split('model')[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c12d65c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"\"\"Please answer with one of the option in the bracket. Write reasoning in between <analysis></analysis>. Write answer in between <answer></answer>.here are the inputs:Q:A 34-year-old man presents to a clinic with complaints of abdominal discomfort and blood in the urine for 2 days. He has had similar abdominal discomfort during the past 5 years, although he does not remember passing blood in the urine. He has had hypertension for the past 2 years, for which he has been prescribed medication. There is no history of weight loss, skin rashes, joint pain, vomiting, change in bowel habits, and smoking. On physical examination, there are ballotable flank masses bilaterally. The bowel sounds are normal. Renal function tests are as follows:\\nUrea 50 mg/dL\\nCreatinine 1.4 mg/dL\\nProtein Negative\\nRBC Numerous\\nThe patient underwent ultrasonography of the abdomen, which revealed enlarged kidneys and multiple anechoic cysts with well-defined walls. A CT scan confirmed the presence of multiple cysts in the kidneys. What is the most likely diagnosis?? \\n{'A': 'Autosomal dominant polycystic kidney disease (ADPKD)', 'B': 'Autosomal recessive polycystic kidney disease (ARPKD)', 'C': 'Medullary cystic disease', 'D': 'Simple renal cysts', 'E': 'Acquired cystic kidney disease'}\"\"\"\n",
    "result = get_completion(query=query, model=merged_model, tokenizer=tokenizer)\n",
    "print(f\"Model Answer : \\n {result.split('model')[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0f2b1e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, PeftModel, AutoPeftModelForCausalLM\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from huggingface_hub import hf_hub_download, hf_hub_url\n",
    "\n",
    "# Set the LoRA configurations\n",
    "peft_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Model ID to use\n",
    "peft_model_id = \"Irathernotsay/qwen2-1.5B-medical_qa-Finetune\"\n",
    "\n",
    "# Verify model existence\n",
    "try:\n",
    "    hf_hub_download(repo_id=peft_model_id)\n",
    "    model_exists = True\n",
    "except Exception as e:\n",
    "    print(f\"Model {peft_model_id} not found: {e}\")\n",
    "    model_exists = False\n",
    "\n",
    "\n",
    "# Load the configuration from the model\n",
    "try:\n",
    "    config = peft_config.from_pretrained(peft_model_id)\n",
    "except ValueError as e:\n",
    "    print(f\"Configuration loading error: {e}\")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(peft_model_id,\n",
    "                                                 return_dict=True,\n",
    "                                                 load_in_4bit=True,\n",
    "                                                 device_map=\"auto\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model or tokenizer: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039ddb3a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141db487",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig,PeftModel,AutoPeftModelForCausalLM\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(secret_value, add_to_git_credential = True)\n",
    "\n",
    "#set the LoRA configurations\n",
    "peft_config =LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "#\n",
    "peft_model_id = \"Irathernotsay/qwen2-1.5B-medical_qa-Finetune\"\n",
    "# config = peft_config.from_pretrained(peft_model_id)\n",
    "#\n",
    "device = \"cuda:0\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(peft_model_id).to(device)\n",
    "ptokenizer = AutoTokenizer.from_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a34992",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_completion(query: str, model, tokenizer) -> str:\n",
    "  device = \"cuda:0\"\n",
    "\n",
    "  prompt_template = \"\"\"\n",
    "  <start_of_turn>user\n",
    "  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "  {query}\n",
    "  <end_of_turn>\\n<start_of_turn>model\n",
    "\n",
    "\n",
    "  \"\"\"\n",
    "  prompt = prompt_template.format(query=query)\n",
    "\n",
    "  encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "  model_inputs = encodeds.to(device)\n",
    "\n",
    "\n",
    "  generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "  # decoded = tokenizer.batch_decode(generated_ids)\n",
    "  decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "  return (decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47c7c2f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"\"\"Hi Doctor, I have a high fever, headache and nausea. What medicines should I take?\\n\"\"\"\n",
    "result = get_completion(query=query, model=model, tokenizer=ptokenizer)\n",
    "print(f\"Model Answer : \\n {result.split('model')[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3852b07",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"\"\"Please answer with one of the option in the bracket. Write reasoning in between <analysis></analysis>. Write answer in between <answer></answer>.here are the inputs:Q:A 34-year-old man presents to a clinic with complaints of abdominal discomfort and blood in the urine for 2 days. He has had similar abdominal discomfort during the past 5 years, although he does not remember passing blood in the urine. He has had hypertension for the past 2 years, for which he has been prescribed medication. There is no history of weight loss, skin rashes, joint pain, vomiting, change in bowel habits, and smoking. On physical examination, there are ballotable flank masses bilaterally. The bowel sounds are normal. Renal function tests are as follows:\\nUrea 50 mg/dL\\nCreatinine 1.4 mg/dL\\nProtein Negative\\nRBC Numerous\\nThe patient underwent ultrasonography of the abdomen, which revealed enlarged kidneys and multiple anechoic cysts with well-defined walls. A CT scan confirmed the presence of multiple cysts in the kidneys. What is the most likely diagnosis?? \\n{'A': 'Autosomal dominant polycystic kidney disease (ADPKD)', 'B': 'Autosomal recessive polycystic kidney disease (ARPKD)', 'C': 'Medullary cystic disease', 'D': 'Simple renal cysts', 'E': 'Acquired cystic kidney disease'}\"\"\"\n",
    "result = get_completion(query=query, model=model, tokenizer=ptokenizer)\n",
    "print(f\"Model Answer : \\n {result.split('model')[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40946c6f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7b5381",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T18:13:02.398835Z",
     "iopub.status.busy": "2024-07-06T18:13:02.398512Z",
     "iopub.status.idle": "2024-07-06T18:24:50.425286Z",
     "shell.execute_reply": "2024-07-06T18:24:50.424126Z",
     "shell.execute_reply.started": "2024-07-06T18:13:02.398809Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import locale\n",
    "def getpreferredencoding(do_setlocale = True):\n",
    "  return \"UTF-8\"\n",
    "locale.getpreferredencoding = getpreferredencoding\n",
    "!git clone https://github.com/ggerganov/llama.cpp\n",
    "!cd llama.cpp && GGML_CUDA=1 make && pip install -r requirements/requirements-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15a415f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T18:24:50.428179Z",
     "iopub.status.busy": "2024-07-06T18:24:50.427832Z",
     "iopub.status.idle": "2024-07-06T18:26:01.166395Z",
     "shell.execute_reply": "2024-07-06T18:26:01.165518Z",
     "shell.execute_reply.started": "2024-07-06T18:24:50.428147Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "model_name = \"Irathernotsay/qwen2-1.5B-medical_qa-Finetune\"\n",
    "methods = ['q4_k_m']\n",
    "base_model = \"./original_model/\"\n",
    "quantized_path = \"./quantized_model/\"\n",
    "#\n",
    "snapshot_download(repo_id=model_name, local_dir=base_model , local_dir_use_symlinks=False)\n",
    "original_model = quantized_path+'/FP16.gguf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76b3eee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T18:29:12.661512Z",
     "iopub.status.busy": "2024-07-06T18:29:12.660766Z",
     "iopub.status.idle": "2024-07-06T18:29:13.655061Z",
     "shell.execute_reply": "2024-07-06T18:29:13.653889Z",
     "shell.execute_reply.started": "2024-07-06T18:29:12.661465Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir ./quantized_model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78f0359",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T18:29:27.011191Z",
     "iopub.status.busy": "2024-07-06T18:29:27.010500Z",
     "iopub.status.idle": "2024-07-06T18:29:39.677083Z",
     "shell.execute_reply": "2024-07-06T18:29:39.675980Z",
     "shell.execute_reply.started": "2024-07-06T18:29:27.011153Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python /kaggle/working/llama.cpp/convert_hf_to_gguf.py ./original_model/ --outtype f16 --outfile ./quantized_model/FP16.gguf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0837978",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T18:32:34.324781Z",
     "iopub.status.busy": "2024-07-06T18:32:34.324091Z",
     "iopub.status.idle": "2024-07-06T18:32:35.314942Z",
     "shell.execute_reply": "2024-07-06T18:32:35.313575Z",
     "shell.execute_reply.started": "2024-07-06T18:32:34.324748Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!chmod +x ./llama.cpp/examples/quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f042560c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T18:32:38.405676Z",
     "iopub.status.busy": "2024-07-06T18:32:38.404716Z",
     "iopub.status.idle": "2024-07-06T18:32:38.412957Z",
     "shell.execute_reply": "2024-07-06T18:32:38.411953Z",
     "shell.execute_reply.started": "2024-07-06T18:32:38.405634Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "for m in methods:\n",
    "  qtype = f\"{quantized_path}/{m.upper()}.gguf\"\n",
    "  os.system(\"./llama.cpp/examples/quantize \"+quantized_path+\"/FP16.gguf \"+qtype+\" \"+m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e749ad",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! ./llama.cpp/quantize ./quantized_model/Q4_K_M.gguf -n 90 --repeat_penalty 1.0 --color -i -r \"User:\" -f llama.cpp/prompts/chat-with-bob.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7eef09",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, HfFolder, create_repo, upload_file\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(secret_value, add_to_git_credential = True)\n",
    "      \n",
    "model_path = \"./Q4_K_M.gguf\" # Your model's local path\n",
    "repo_name = \"qwen2-1.5b-medical_qa-GGUF\"  # Desired HF Hub repository name\n",
    "repo_url = create_repo(repo_name, private=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47adb19e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    path_or_fileobj=model_path,\n",
    "    path_in_repo=\"/kaggle/working/quantized_model/FP16.gguf\",\n",
    "    repo_id=\"Irathernotsay/qwen2-1.5B-medical_qa-Finetune-GGUF\",\n",
    "    repo_type=\"model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3c5fa7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget \"https://huggingface.co/Plaban81/gemma-medical_qa-GGUF/resolve/main/Q4_K_M.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b97a6e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290055f6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "llm = Llama(\n",
    "  model_path=\"/content/Q4_K_M.gguf\",  # Download the model file first\n",
    "  n_ctx=32768,  # The max sequence length to use - note that longer sequence lengths require much more resources\n",
    "  n_threads=1,            # The number of CPU threads to use, tailor to your system and the resulting performance\n",
    "  n_gpu_layers=-1         # The number of layers to offload to GPU, if you have GPU acceleration available\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aada953",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"\"\"Please answer with one of the option in the bracket. Write reasoning in between <analysis></analysis>. Write answer in between <answer></answer>. here are the inputs Q:An 8-year-old boy is brought to the pediatrician by his mother with nausea, vomiting, and decreased frequency of urination. He has acute lymphoblastic leukemia for which he received the 1st dose of chemotherapy 5 days ago. His leukocyte count was 60,000/mm3 before starting chemotherapy. The vital signs include: pulse 110/min, temperature 37.0°C (98.6°F), and blood pressure 100/70 mm Hg. The physical examination shows bilateral pedal edema. Which of the following serum studies and urinalysis findings will be helpful in confirming the diagnosis of this condition? ? \\n{'A': 'Hyperkalemia, hyperphosphatemia, hypocalcemia, and extremely elevated creatine kinase (MM)', 'B': 'Hyperkalemia, hyperphosphatemia, hypocalcemia, hyperuricemia, urine supernatant pink, and positive for heme', 'C': 'Hyperuricemia, hyperkalemia, hyperphosphatemia, lactic acidosis, and urate crystals in the urine', 'D': 'Hyperuricemia, hyperkalemia, hyperphosphatemia, and urinary monoclonal spike', 'E': 'Hyperuricemia, hyperkalemia, hyperphosphatemia, lactic acidosis, and oxalate crystals'}\"\"\"\n",
    "output = llm(\n",
    "  prompt=query,\n",
    "  max_tokens=512,  # Generate up to 512 tokens\n",
    ")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9199a23",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\\n\\n Please answer with one of the option in the bracket. Write reasoning in between <analysis></analysis>. Write answer in between <answer></answer>. here are the inputs Q:An 8-year-old boy is brought to the pediatrician by his mother with nausea, vomiting, and decreased frequency of urination. He has acute lymphoblastic leukemia for which he received the 1st dose of chemotherapy 5 days ago. His leukocyte count was 60,000/mm3 before starting chemotherapy. The vital signs include: pulse 110/min, temperature 37.0°C (98.6°F), and blood pressure 100/70 mm Hg. The physical examination shows bilateral pedal edema. Which of the following serum studies and urinalysis findings will be helpful in confirming the diagnosis of this condition? ? \\n{'A': 'Hyperkalemia, hyperphosphatemia, hypocalcemia, and extremely elevated creatine kinase (MM)', 'B': 'Hyperkalemia, hyperphosphatemia, hypocalcemia, hyperuricemia, urine supernatant pink, and positive for heme', 'C': 'Hyperuricemia, hyperkalemia, hyperphosphatemia, lactic acidosis, and urate crystals in the urine', 'D': 'Hyperuricemia, hyperkalemia, hyperphosphatemia, and urinary monoclonal spike', 'E': 'Hyperuricemia, hyperkalemia, hyperphosphatemia, lactic acidosis, and oxalate crystals'}\"\"\"\n",
    "output = llm(\n",
    "  prompt=query,\n",
    "  max_tokens=512,  # Generate up to 512 tokens\n",
    ")\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ff14ab",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(output[\"choices\"][0][\"text\"].split(\"<end_of_turn>\\n<end_of_turn>model\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db3c8a3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 25.216883,
   "end_time": "2024-08-23T01:22:58.011767",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-23T01:22:32.794884",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
